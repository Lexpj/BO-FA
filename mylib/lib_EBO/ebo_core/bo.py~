import numpy as np
from ebo_core.gibbs import GibbsSampler
from scipy.optimize import minimize

from scipy.stats import norm



class bo(object):
    def __init__(self, f, X, y, x_range, eval_only, extra, options):
        self.f = f
        self.options = options
        self.eval_only = eval_only
        self.X = X
        self.y = y

        if eval_only:
            self.newX = extra
        else:
            self.x_range = x_range

            self.well_defined = X.shape[0] > 0
            self.solver = GibbsSampler(X, y, options)

            self.opt_n = options['opt_n']
            self.dx = options['dx']
            self.n_bo = extra
            self.opt_n = np.maximum(self.opt_n, self.n_bo * 2)

        self.max_value = self.options['max_value']
        self.n_bo_top_percent = self.options['n_bo_top_percent']

    def learn(self):
        self.gp, self.z, self.k = self.solver.run(self.options['gibbs_iter'])

    # def run(self):
    #     if self.eval_only:
    #         ynew = [self.f(x) for x in self.newX]
    #         return ynew
    #
    #     # return random inputs if X is empty
    #     if not self.well_defined:
    #         xnew = np.random.uniform(self.x_range[0], self.x_range[1], (self.n_bo, self.dx))
    #         acfnew = [self.max_value] * self.n_bo
    #         return xnew, acfnew, self.solver.z, self.solver.k
    #
    #     # learn and optimize
    #     self.learn()
    #     # initialization
    #     xnew = np.empty((self.n_bo, self.dx))
    #     xnew[0] = np.random.uniform(self.x_range[0], self.x_range[1])
    #     # optimize group by group
    #     all_cat = np.unique(self.z)
    #     for a in np.random.permutation(all_cat):
    #         active = self.z == a
    #         af = lambda x: acfun(x, xnew[0], active, self.max_value, self.gp)
    #         xnew[:, active] = global_minimize(af, self.x_range[:, active], \
    #                                           self.opt_n, self.n_bo, self.n_bo_top_percent)
    #     mu, var = self.gp.predict(xnew)
    #     acfnew = np.squeeze((self.max_value - mu) / np.sqrt(var))
    #     return xnew, acfnew, self.z, self.k

    def run(self):
        if self.eval_only:
            ynew = [self.f(x) for x in self.newX]
            return ynew

        # return random inputs if X is empty
        if not self.well_defined:
            xnew = np.random.uniform(self.x_range[0], self.x_range[1], (self.n_bo, self.dx))
            acfnew = [self.max_value] * self.n_bo
            return xnew, acfnew, self.solver.z, self.solver.k

        # learn and optimize
        self.learn()
        # initialization
        xnew = np.empty((self.n_bo, self.dx))
        xnew[0] = np.random.uniform(self.x_range[0], self.x_range[1])
        # optimize group by group
        all_cat = np.unique(self.z)
        for a in np.random.permutation(all_cat):
            active = self.z == a
            cur = self.y.argmax(axis=0)
            y_star = self.y[cur]
            # breakpoint)
            EI_weight = 0.5
            af = lambda x: acfun(x, xnew[0], active, EI_weight, self.gp, y_star)
            xnew[:, active] = global_minimize(af, self.x_range[:, active], \
                                              self.opt_n, self.n_bo, self.n_bo_top_percent)
        mu, var = self.gp.predict(xnew)

        if EI_weight == 0.5:  # to be consitent with definition of EI not weighted EI
            acfnew = np.squeeze(fterm + sterm)
        else:
            acfnew = np.squeeze(w * fterm + (1 - w) * sterm)
        
        return xnew, acfnew, self.z, self.k

def global_minimize(f, x_range, n, n_bo=1, n_bo_top_percent=1.0):
    dx = x_range.shape[1]
    tx = np.random.uniform(x_range[0], x_range[1], (n, dx))
    ty = f(tx)
    x0 = tx[ty.argmin()]  # x0 is a 2d array of size 1*dx
    res = minimize(f, x0, bounds=x_range.T, method='L-BFGS-B')
    tx = np.vstack((tx, res.x))
    ty = np.hstack((ty, res.fun))
    inds = ty.argsort()
    thres = np.ceil(n_bo * n_bo_top_percent).astype(int)
    inds_of_inds = np.hstack((range(thres), np.random.permutation(range(thres, len(inds)))))
    inds = inds[inds_of_inds[:n_bo]]
    return tx[inds, :]

# def acfun(X, fixX, active_dims, maxval, gp, b):
#
#     if len(X.shape) > 1:
#         nX = np.matlib.repmat(fixX, X.shape[0], 1)
#         nX[:, active_dims] = X
#     else:
#         nX = fixX
#         nX[active_dims] = X
#     mu, var = gp.predict(nX)
#     assert (var > 0).all(), 'error in acfun: variance <= 0??'
#
#     return np.squeeze((maxval - mu) / np.sqrt(var))

def acfun(X, fixX, active_dims, w, gp, y_min):

    if len(X.shape) > 1:
        nX = np.matlib.repmat(fixX, X.shape[0], 1)
        nX[:, active_dims] = X
    else:
        nX = fixX
        nX[active_dims] = X
    mu, var = gp.predict(nX)
    assert (var > 0).all(), 'error in acfun: variance <= 0??'
    if w > 1 or w < 0:
        raise Exception("Weight should be in [0,1]")
    fterm = (y_min - mu) * norm.cdf((y_min - mu) / var)
    sterm = var * norm.pdf((y_min - mu / var))
    if w == 0.5:  # to be consitent with definition of EI not weighted EI
        return np.squeeze(fterm + sterm)
    else:
        return np.squeeze(w * fterm + (1 - w) * sterm)

# def acfun_prova(X, fixX, active_dims, gp, w = 0.5, bo()):
#     if len(X.shape) > 1:
#     nX = np.matlib.repmat(fixX, X.shape[0], 1)
#     nX[:, active_dims] = X
#     else:
#         nX = fixX
#         nX[active_dims] = X
#     mu, var = gp.predict(nX)
#     assert (var > 0).all(), 'error in acfun: variance <= 0??'
#
#     #y_min = np.min(ys)  # best observed value sofar (lowest output)
#     if w > 1 or w < 0:
#         raise Exception("Weight should be in [0,1]")
#     if var == 0:
#         return 0
#     else:
#         fterm = (y_min - mu) * norm.cdf((y_min - mu) / var)
#         sterm = var * norm.pdf((y_min - mu / var)
#         if w == 0.5:  # to be consitent with definition of EI not weighted EI
#             return fterm + sterm
#         else:
#             return w * fterm + (1 - w) * sterm
#
#     return exptimp
#
#
# def acfun_prova(X, fixX, active_dims, maxval, gp):
#     if len(X.shape) > 1:
#         nX = np.matlib.repmat(fixX, X.shape[0], 1)
#         nX[:, active_dims] = X
#     else:
#         nX = fixX
#         nX[active_dims] = X
#     mu, var = gp.predict(nX)
#     assert (var > 0).all(), 'error in acfun: variance <= 0??'
#     w=0.5
#     y_min = np.min(ys)  # best observed value sofar (lowest output)
#
#     fterm = (y_min - y_hat) * norm.cdf((y_min - y_hat) / var)
#     sterm = var * norm.pdf((y_min - y_hat) / var)
#     if w == 0.5:  # to be consitent with definition of EI not weighted EI
#         return fterm + sterm
#     else:
#         return w * fterm + (1 - w) * sterm
#
#     return exptimp
#
# def acfun_prova(X, fixX,active_dims, maxval, gp):
#     """ expected_improvement
#     Expected improvement acquisition function.
#     Arguments:
#     ----------
#         x: array-like, shape = [n_samples, n_hyperparams]
#             The point for which the expected improvement needs to be computed.
#         gaussian_process: GaussianProcessRegressor object.
#             Gaussian process trained on previously evaluated hyperparameters.
#         evaluated_loss: Numpy array.
#             Numpy array that contains the values off the loss function for the previously
#             evaluated hyperparameters.
#         greater_is_better: Boolean.
#             Boolean flag that indicates whether the loss function is to be maximised or minimised.
#         n_params: int.
#             Dimension of the hyperparameter space.
#     """
#     n_params=1
#     x_to_predict = X.reshape(-1, n_params)
#
#     mu, sigma = gp.predict(x_to_predict, return_std=True)
#     greater_is_better=False
#     if greater_is_better:
#         loss_optimum = np.max(evaluated_loss)
#     else:
#         loss_optimum = np.min(evaluated_loss)
#
#     scaling_factor = (-1) ** (not greater_is_better)
#
#     # In case sigma equals zero
#     with np.errstate(divide='ignore'):
#         Z = scaling_factor * (mu - loss_optimum) / sigma
#         expected_improvement = scaling_factor * (mu - loss_optimum) * norm.cdf(Z) + sigma * norm.pdf(Z)
#         expected_improvement[sigma == 0.0] == 0.0
#
#     return -1 * expected_improvement
#
# def acqufun(X,fixX,active, active_dims, maxval, gp):
#     if len(X.shape) > 1:
#         nX = np.matlib.repmat(fixX, X.shape[0], 1)
#         nX[:, active_dims] = X
#     else:
#         nX = fixX
#         nX[active_dims] = X
#     mu, var = gp.predict(nX)
#
#
#
#     sigma = var.reshape(-1, 1)
#
#     # Needed for noise-based model,
#     # otherwise use np.max(Y_sample).
#     # See also section 2.4 in [1]
#     mu_opt = np.max(mu)
#     xi = 0.01
#     with np.errstate(divide='warn'):
#         imp = mu - mu_sample_opt - xi
#         Z = imp / sigma
#         ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)
#         ei[sigma == 0.0] = 0.0
#
#     return ei
